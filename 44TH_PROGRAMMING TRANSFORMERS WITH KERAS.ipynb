{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDaVUCyp2t7"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 10: Time Series in Keras**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wj906IRp2t8"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
        "  Running the following code will map your GDrive to ```/content/drive```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAdYRJcvp2t8",
        "outputId": "aee346d2-af87-4639-cf1d-66634affd0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riFjHKOU22aO"
      },
      "source": [
        "# Part 10.5: Programming Transformers with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14QUyFpKp2uA"
      },
      "source": [
        "This section shows an example of a transformer encoder to predict sunspots.  You can find the data files needed for this example at the following location.\n",
        "\n",
        "* [Sunspot Data Files](http://www.sidc.be/silso/datafiles#total)\n",
        "* [Download Daily Sunspots](http://www.sidc.be/silso/INFO/sndtotcsv.php) - 1/1/1818 to now.\n",
        "\n",
        "The following code loads the sunspot file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u12-bjaOp2uA",
        "outputId": "20d3ba09-3b20-4a2d-8db2-926514728aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting file:\n",
            "   year  month  day  dec_year  sn_value  sn_error  obs_num  extra\n",
            "0  1818      1    1  1818.001        -1       NaN        0      1\n",
            "1  1818      1    2  1818.004        -1       NaN        0      1\n",
            "2  1818      1    3  1818.007        -1       NaN        0      1\n",
            "3  1818      1    4  1818.010        -1       NaN        0      1\n",
            "4  1818      1    5  1818.012        -1       NaN        0      1\n",
            "5  1818      1    6  1818.015        -1       NaN        0      1\n",
            "6  1818      1    7  1818.018        -1       NaN        0      1\n",
            "7  1818      1    8  1818.021        65      10.2        1      1\n",
            "8  1818      1    9  1818.023        -1       NaN        0      1\n",
            "9  1818      1   10  1818.026        -1       NaN        0      1\n",
            "Ending file:\n",
            "       year  month  day  dec_year  sn_value  sn_error  obs_num  extra\n",
            "72855  2017      6   21  2017.470        35       1.0       41      0\n",
            "72856  2017      6   22  2017.473        24       0.8       39      0\n",
            "72857  2017      6   23  2017.475        23       0.9       40      0\n",
            "72858  2017      6   24  2017.478        26       2.3       15      0\n",
            "72859  2017      6   25  2017.481        17       1.0       18      0\n",
            "72860  2017      6   26  2017.484        21       1.1       25      0\n",
            "72861  2017      6   27  2017.486        19       1.2       36      0\n",
            "72862  2017      6   28  2017.489        17       1.1       22      0\n",
            "72863  2017      6   29  2017.492        12       0.5       25      0\n",
            "72864  2017      6   30  2017.495        11       0.5       30      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "  \n",
        "names = ['year', 'month', 'day', 'dec_year', 'sn_value' , \n",
        "         'sn_error', 'obs_num', 'extra']\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n",
        "    sep=';',header=None,names=names,\n",
        "    na_values=['-1'], index_col=False)\n",
        "\n",
        "print(\"Starting file:\")\n",
        "print(df[0:10])\n",
        "\n",
        "print(\"Ending file:\")\n",
        "print(df[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzV1XGS0p2uA"
      },
      "source": [
        "As you can see, there is quite a bit of missing data near the end of the file.  We want to find the starting index where the missing data no longer occurs.  This technique is somewhat sloppy; it would be better to find a use for the data between missing values.  However, the point of this example is to show how to use a transformer encoder with a somewhat simple time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2pQeFAzp2uA",
        "outputId": "c97df582-c332-43fc-c81f-a19eb73c1b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11314\n"
          ]
        }
      ],
      "source": [
        "# Find the last zero and move one beyond\n",
        "start_id = max(df[df['obs_num'] == 0].index.tolist())+1  \n",
        "print(start_id)\n",
        "df = df[start_id:] # Trim the rows that have missing observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNZcj-s6Vp2R"
      },
      "source": [
        "Divide into training and test/validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBaewsY_p2uB",
        "outputId": "050d50f2-1f71-4c19-da70-0d12b6672890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 55160 observations.\n",
            "Test set has 6391 observations.\n"
          ]
        }
      ],
      "source": [
        "df['sn_value'] = df['sn_value'].astype(float)\n",
        "df_train = df[df['year']<2000]\n",
        "df_test = df[df['year']>=2000]\n",
        "\n",
        "spots_train = df_train['sn_value'].tolist()\n",
        "spots_test = df_test['sn_value'].tolist()\n",
        "\n",
        "print(\"Training set has {} observations.\".format(len(spots_train)))\n",
        "print(\"Test set has {} observations.\".format(len(spots_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9P7RnqYQzh"
      },
      "source": [
        "The **to_sequences** function takes linear time series data into an **x** and **y** where **x** is all possible sequences of seq_size. After each **x** sequence, this function places the next value into the **y** variable. These **x** and **y** data can train a time-series neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaltDADPp2uB",
        "outputId": "b99166d3-16bb-4131-aa08-a7f97b138660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training set: (55150, 10, 1)\n",
            "Shape of test set: (6381, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_sequences(seq_size, obs):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
        "        #print(i)\n",
        "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
        "        after_window = obs[i+SEQUENCE_SIZE]\n",
        "        window = [[x] for x in window]\n",
        "        #print(\"{} - {}\".format(window,after_window))\n",
        "        x.append(window)\n",
        "        y.append(after_window)\n",
        "        \n",
        "    return np.array(x),np.array(y)\n",
        "    \n",
        "    \n",
        "SEQUENCE_SIZE = 10\n",
        "x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
        "x_test,y_test = to_sequences(SEQUENCE_SIZE,spots_test)\n",
        "\n",
        "print(\"Shape of training set: {}\".format(x_train.shape))\n",
        "print(\"Shape of test set: {}\".format(x_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDjLR0RjYl5y"
      },
      "source": [
        "We can view the results of the **to_sequences** encoding of the sunspot data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNH3BG-jp2uB",
        "outputId": "7876c4e8-7e1e-4d56-9c6f-88d69a609c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55150, 10, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlmXgzbcZEHn"
      },
      "source": [
        "Next, we create the transformer_encoder; I obtained this function from a [Keras example](https://keras.io/examples/timeseries/timeseries_transformer_classification/). This layer includes residual connections, layer normalization, and dropout. This resulting layer can be stacked multiple times. We implement the projection layers with the Keras Conv1D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YZ5ltq1L397v"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2peKf3Z4H_"
      },
      "source": [
        "The following function is provided to build the model, including the attention layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HUAVPayh4MKc"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkxS7ZpXc_Tf"
      },
      "source": [
        "We are now ready to build and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzacEsJip2uB",
        "outputId": "a67fd712-1398-42ef-f20d-6567f243fcb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "690/690 [==============================] - 99s 137ms/step - loss: 2161.7170 - val_loss: 462.6251\n",
            "Epoch 2/200\n",
            "690/690 [==============================] - 94s 136ms/step - loss: 1187.9396 - val_loss: 401.3827\n",
            "Epoch 3/200\n",
            "690/690 [==============================] - 96s 139ms/step - loss: 943.8872 - val_loss: 356.0738\n",
            "Epoch 4/200\n",
            "690/690 [==============================] - 94s 137ms/step - loss: 832.2631 - val_loss: 330.3765\n",
            "Epoch 5/200\n",
            "690/690 [==============================] - 96s 139ms/step - loss: 798.6705 - val_loss: 331.5389\n",
            "Epoch 6/200\n",
            "690/690 [==============================] - 94s 137ms/step - loss: 779.3980 - val_loss: 314.3635\n",
            "Epoch 7/200\n",
            "690/690 [==============================] - 96s 139ms/step - loss: 761.3097 - val_loss: 306.0876\n",
            "Epoch 8/200\n",
            "690/690 [==============================] - 96s 139ms/step - loss: 749.4420 - val_loss: 300.9117\n",
            "Epoch 9/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 737.4802 - val_loss: 311.7221\n",
            "Epoch 10/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 738.6097 - val_loss: 311.1595\n",
            "Epoch 11/200\n",
            "690/690 [==============================] - 93s 135ms/step - loss: 730.5956 - val_loss: 298.5232\n",
            "Epoch 12/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 721.9236 - val_loss: 301.0043\n",
            "Epoch 13/200\n",
            "690/690 [==============================] - 95s 137ms/step - loss: 717.2314 - val_loss: 296.8640\n",
            "Epoch 14/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 725.4708 - val_loss: 291.8970\n",
            "Epoch 15/200\n",
            "690/690 [==============================] - 93s 135ms/step - loss: 714.7941 - val_loss: 308.9366\n",
            "Epoch 16/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 722.6943 - val_loss: 312.1275\n",
            "Epoch 17/200\n",
            "690/690 [==============================] - 94s 136ms/step - loss: 715.1848 - val_loss: 306.9655\n",
            "Epoch 18/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 710.5483 - val_loss: 299.8122\n",
            "Epoch 19/200\n",
            "690/690 [==============================] - 93s 135ms/step - loss: 708.7395 - val_loss: 297.5239\n",
            "Epoch 20/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 708.3118 - val_loss: 313.8019\n",
            "Epoch 21/200\n",
            "690/690 [==============================] - 93s 135ms/step - loss: 707.7223 - val_loss: 317.8582\n",
            "Epoch 22/200\n",
            "690/690 [==============================] - 95s 138ms/step - loss: 702.1491 - val_loss: 295.7321\n",
            "Epoch 23/200\n",
            "690/690 [==============================] - 94s 136ms/step - loss: 708.4930 - val_loss: 305.1639\n",
            "Epoch 24/200\n",
            "690/690 [==============================] - 97s 141ms/step - loss: 701.3448 - val_loss: 295.4283\n",
            "200/200 [==============================] - 5s 27ms/step - loss: 217.0773\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217.07733154296875"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"mean_squared_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "#model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
        "    restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.evaluate(x_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jeJf5zsp2uB"
      },
      "source": [
        "Finally, we evaluate the model with RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqiV9Oq2p2uB",
        "outputId": "f38b013e-9bbb-48a7-a7a3-c5b613b59644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200/200 [==============================] - 6s 29ms/step\n",
            "Score (RMSE): 14.73354408380693\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Score (RMSE): {}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "t81_558_class_10_5_keras_transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}